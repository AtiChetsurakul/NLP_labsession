{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentiment Analysis with twitter API\n",
                "\n",
                "In this work, we want to find if the tag in twitter (e.g. \"iphone) is positive or negative by using\n",
                "sentiment analysis on the post related to tag. Then use our model to determine if it contains positive or negative sentiment."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data preparation\n",
                "\n",
                "Just loading and do some indexing. Nothing much."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "cpu\n"
                    ]
                }
            ],
            "source": [
                "# Import necessary libraries.\n",
                "import torch\n",
                "from torch import nn\n",
                "import time\n",
                "\n",
                "# Check if we can use CUDA\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(device)\n",
                "\n",
                "# Use for reproducability\n",
                "SEED = 1234\n",
                "torch.manual_seed(SEED)\n",
                "torch.backends.cudnn.deterministic = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pytreebank\n",
                "dataset = pytreebank.load_sst()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def unpack(data):\n",
                "    temp_data  = list()\n",
                "    temp_label = list()\n",
                "    lenght = len(list(iter(data)))\n",
                "    for i in range(lenght):\n",
                "        for label, sentence in data[i].to_labeled_lines():\n",
                "            temp_data.append(sentence)\n",
                "            temp_label.append(label)\n",
                "    \n",
                "    return (temp_data, temp_label)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "8544 1101 2210\n",
                        "318582 41447 82600\n",
                        "318582 41447 82600\n"
                    ]
                }
            ],
            "source": [
                "# We do not want to use the treebank structure.\n",
                "# Seperated into train, dev and test set\n",
                "train_data, train_label = unpack(dataset[\"train\"])\n",
                "valid_data, valid_label = unpack(dataset[\"dev\"])\n",
                "test_data,  test_label  = unpack(dataset[\"test\"])\n",
                "\n",
                "# Check the lenght of each data set\n",
                "train_size = len(train_data)\n",
                "valid_size = len(valid_data)\n",
                "test_size =  len(test_data)\n",
                "\n",
                "# This is the same as train_data above, it is just in the format of tree.\n",
                "# I do this for the sake of convenience when we trying to build dataloader.\n",
                "# But IT IS NOT A GOOD PRACTICE!\n",
                "train = dataset[\"train\"]\n",
                "valid = dataset[\"dev\"]\n",
                "test  = dataset[\"test\"]\n",
                "\n",
                "t_train_size = len(list(iter(train)))\n",
                "t_valid_size = len(list(iter(valid)))\n",
                "t_test_size = len(list(iter(test)))\n",
                "\n",
                "print(t_train_size, t_valid_size, t_test_size)\n",
                "print(train_size, valid_size, test_size)\n",
                "print(len(train_label), len(valid_label), len(test_label))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\",\n",
                            " 3)"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Let's take a look\n",
                "train_data[0], train_label[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tokenize\n",
                "from torchtext.data.utils import get_tokenizer\n",
                "\n",
                "tokenizer = get_tokenizer('spacy', language='en_core_web_md')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Numericalization\n",
                "\n",
                "from torchtext.vocab import build_vocab_from_iterator\n",
                "\n",
                "def yield_tokens(data_iter):  #data_iter = train, test, validation\n",
                "    for data in data_iter:  # Look for the tree\n",
                "        for _, text in data.to_labeled_lines(): # Get the data inside tree\n",
                "            yield tokenizer(text)\n",
                "        \n",
                "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>',\n",
                "                                                                 '<bos>', '<eos>'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[0, 919, 36, 2733, 9, 28, 908, 3233, 10]\n",
                        "[1, 2, 3]\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "17136"
                        ]
                    },
                    "execution_count": 44,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "vocab.set_default_index(vocab[\"<unk>\"])\n",
                "\n",
                "# Check if our vocab is working.\n",
                "print(vocab(['Chaky', 'wants', 'his', 'student', 'to', 'be', 'number', '1', '.']))\n",
                "print(vocab(['<pad>','<bos>','<eos>']))\n",
                "id2word = vocab.get_itos()\n",
                "id2word[0]\n",
                "\n",
                "len(vocab)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare Embedding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchtext.vocab import FastText\n",
                "fast_vectors = FastText(language='simple')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Now that we get the vectors, it's time to create embedding.\n",
                "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([17136, 300])"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Let check the shape\n",
                "fast_embedding.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare Dataloader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
                "#label_pipeline = lambda x: int(x) - 1  #1, 2, 3, 4 ---> 0, 1, 2, 3 #"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[63, 110, 9, 494, 8735]"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Testing text_pipeline\n",
                "text_pipeline(\"I love to play football\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "3"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "t = torch.empty(3, 4, 5)\n",
                "t.size()\n",
                "torch.Size([3, 4, 5])\n",
                "t.size(0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader\n",
                "from torch.nn.utils.rnn import pad_sequence #making each batch same length\n",
                "\n",
                "pad_ix = vocab['<pad>']\n",
                "\n",
                "#this function gonna be called by DataLoader\n",
                "def collate_batch(batch):\n",
                "    label_list, text_list, length_list = [], [], []\n",
                "    for (_label, _text) in batch:\n",
                "        label_list.append(_label)\n",
                "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
                "        text_list.append(processed_text)\n",
                "        length_list.append(processed_text.size(0)) #for padding, this keep the lenght of sequence.\n",
                "        \n",
                "    return torch.tensor(label_list, dtype=torch.int64), \\\n",
                "        pad_sequence(text_list, padding_value=pad_ix, batch_first=True), \\\n",
                "        torch.tensor(length_list, dtype=torch.int64)  # The pad_seq functions automatically do the work."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We need the data in the format of tuples .. e.g. (label, text)\n",
                "\n",
                "def merge(list1, list2):\n",
                "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
                "    return merged_list\n",
                "\n",
                "\n",
                "training_data   = merge(train_label, train_data)\n",
                "validation_data = merge(valid_label, valid_data)\n",
                "testing_data    = merge(test_label,  test_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(3,\n",
                            " \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\")"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Test the one that we created.\n",
                "training_data[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(3,\n",
                            " \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\")"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# The one that we already have.\n",
                "# Exactly the same!\n",
                "train_label[0], train_data[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 64\n",
                "\n",
                "train_loader = DataLoader(training_data, batch_size = batch_size,\n",
                "                          shuffle=True, collate_fn=collate_batch)\n",
                "\n",
                "val_loader   = DataLoader(validation_data, batch_size = batch_size,\n",
                "                          shuffle=True, collate_fn=collate_batch)\n",
                "\n",
                "test_loader  = DataLoader(testing_data, batch_size = batch_size,\n",
                "                          shuffle=True, collate_fn=collate_batch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Label shape:  torch.Size([64])\n",
                        "Text shape:  torch.Size([64, 26])\n"
                    ]
                }
            ],
            "source": [
                "for label, text, length in train_loader:\n",
                "    break\n",
                "print(\"Label shape: \", label.shape) # (batch_size, )\n",
                "print(\"Text shape: \", text.shape)   # (batch_size, seq len)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare Model\n",
                "Basically in this part, we will just define LSTM neural network and function for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "\n",
                "class LSTM(nn.Module):\n",
                "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
                "        super().__init__()\n",
                "        #put padding_idx so asking the embedding layer to ignore padding\n",
                "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_ix)\n",
                "        self.lstm = nn.LSTM(emb_dim, \n",
                "                           hid_dim, \n",
                "                           num_layers=num_layers, \n",
                "                           bidirectional=bidirectional, \n",
                "                           dropout=dropout,\n",
                "                           batch_first=True)\n",
                "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
                "        \n",
                "    def forward(self, text, text_lengths):\n",
                "        #text = [batch size, seq len]\n",
                "        embedded = self.embedding(text)\n",
                "        \n",
                "        #++ pack sequence ++\n",
                "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
                "        \n",
                "        #embedded = [batch size, seq len, embed dim]\n",
                "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
                "        \n",
                "        #++ unpack in case we need to use it ++\n",
                "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
                "        \n",
                "        #output = [batch size, seq len, hidden dim * num directions]\n",
                "        #output over padding tokens are zero tensors\n",
                "        \n",
                "        #hidden = [num layers * num directions, batch size, hid dim]\n",
                "        #cell = [num layers * num directions, batch size, hid dim]\n",
                "        \n",
                "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
                "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
                "        #hn = [batch size, hidden dim * num directions]\n",
                "        \n",
                "        return self.fc(hn)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "#explicitly initialize weights for better learning\n",
                "def initialize_weights(m):\n",
                "    if isinstance(m, nn.Linear):\n",
                "        nn.init.xavier_normal_(m.weight)\n",
                "        nn.init.zeros_(m.bias)\n",
                "    elif isinstance(m, nn.LSTM):\n",
                "        for name, param in m.named_parameters():\n",
                "            if 'bias' in name:\n",
                "                nn.init.zeros_(param)\n",
                "            elif 'weight' in name:\n",
                "                nn.init.orthogonal_(param)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_dim  = len(vocab)\n",
                "hid_dim    = 256\n",
                "emb_dim    = 300         # Why 300, we do not know depend on you.\n",
                "output_dim = 5 # [0, 1, 2, 3, 4] # We have 5 class\n",
                "\n",
                "#for biLSTM\n",
                "num_layers = 2\n",
                "bidirectional = True\n",
                "dropout = 0.5\n",
                "\n",
                "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
                "model.apply(initialize_weights)\n",
                "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "5140800\n",
                        "307200\n",
                        "262144\n",
                        "  1024\n",
                        "  1024\n",
                        "307200\n",
                        "262144\n",
                        "  1024\n",
                        "  1024\n",
                        "524288\n",
                        "262144\n",
                        "  1024\n",
                        "  1024\n",
                        "524288\n",
                        "262144\n",
                        "  1024\n",
                        "  1024\n",
                        "  2560\n",
                        "     5\n",
                        "______\n",
                        "7863109\n"
                    ]
                }
            ],
            "source": [
                "#we can print the complexity by the number of parameters\n",
                "def count_parameters(model):\n",
                "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
                "    for item in params:\n",
                "        print(f'{item:>6}')\n",
                "    print(f'______\\n{sum(params):>6}')\n",
                "    \n",
                "count_parameters(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.optim as optim\n",
                "\n",
                "lr=1e-3\n",
                "\n",
                "#training hyperparameters\n",
                "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "def accuracy(preds, y):\n",
                "    \n",
                "    predicted = torch.max(preds.data, 1)[1]\n",
                "    batch_corr = (predicted == y).sum()\n",
                "    acc = batch_corr / len(y)\n",
                "    \n",
                "    return acc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model, loader, optimizer, criterion, loader_length):\n",
                "    epoch_loss = 0\n",
                "    epoch_acc = 0\n",
                "    model.train() #useful for batchnorm and dropout\n",
                "    \n",
                "    for i, (label, text, text_length) in enumerate(loader): \n",
                "        label = label.to(device) #(batch_size, )\n",
                "        text = text.to(device) #(batch_size, seq len)\n",
                "                \n",
                "        #predict\n",
                "        predictions = model(text, text_length).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
                "        \n",
                "        #calculate loss\n",
                "        loss = criterion(predictions, label)\n",
                "        acc  = accuracy(predictions, label)\n",
                "        \n",
                "        #backprop\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        epoch_loss += loss.item()\n",
                "        epoch_acc += acc.item()\n",
                "                        \n",
                "    return epoch_loss / loader_length, epoch_acc / loader_length"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(model, loader, criterion, loader_length):\n",
                "    epoch_loss = 0\n",
                "    epoch_acc = 0\n",
                "    model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for i, (label, text, text_length) in enumerate(loader): \n",
                "            label = label.to(device) #(batch_size, )\n",
                "            text  = text.to(device)  #(seq len, batch_size)\n",
                "\n",
                "            predictions = model(text, text_length).squeeze(1) \n",
                "            \n",
                "            loss = criterion(predictions, label)\n",
                "            acc  = accuracy(predictions, label)\n",
                "\n",
                "            epoch_loss += loss.item()\n",
                "            epoch_acc += acc.item()\n",
                "        \n",
                "    return epoch_loss / loader_length, epoch_acc / loader_length"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to calculate time.\n",
                "def epoch_time(start_time, end_time):\n",
                "    elapsed_time = end_time - start_time\n",
                "    elapsed_mins = int(elapsed_time / 60)\n",
                "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
                "    return elapsed_mins, elapsed_secs"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_loader_length = len(list(iter(train_loader)))\n",
                "val_loader_length   = len(list(iter(val_loader)))\n",
                "test_loader_length  = len(list(iter(test_loader)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[37], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     14\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 16\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, optimizer, criterion, train_loader_length)\n\u001b[1;32m     17\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m evaluate(model, val_loader, criterion, val_loader_length)\n\u001b[1;32m     19\u001b[0m     \u001b[39m#for plotting\u001b[39;00m\n",
                        "Cell \u001b[0;32mIn[33], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, loader_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m#backprop\u001b[39;00m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
                        "File \u001b[0;32m~/opt/anaconda3/envs/ai50/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
                        "File \u001b[0;32m~/opt/anaconda3/envs/ai50/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "best_valid_loss = float('inf')\n",
                "num_epochs      = 8\n",
                "tolerance_counter = 0\n",
                "\n",
                "save_path = f'/root/projects/NLP/Assignment/8_Feb_Sentiment_Analysis/weights/{model.__class__.__name__}.pt'\n",
                "\n",
                "train_losses = []\n",
                "train_accs = []\n",
                "valid_losses = []\n",
                "valid_accs = []\n",
                "\n",
                "for epoch in range(num_epochs):\n",
                "    \n",
                "    start_time = time.time()\n",
                "\n",
                "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
                "    valid_loss, valid_acc = evaluate(model, val_loader, criterion, val_loader_length)\n",
                "    \n",
                "    #for plotting\n",
                "    train_losses.append(train_loss)\n",
                "    train_accs.append(train_acc)\n",
                "    valid_losses.append(valid_loss)\n",
                "    valid_accs.append(valid_acc)\n",
                "    \n",
                "    end_time = time.time()\n",
                "    \n",
                "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
                "    \n",
                "    if valid_loss < best_valid_loss:\n",
                "        best_valid_loss = valid_loss\n",
                "        tolerance_counter = 0\n",
                "        torch.save(model.state_dict(), save_path)\n",
                "    \n",
                "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
                "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')   \n",
                "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
                "\n",
                "    # Tolerance techniques, stop the model if it start to overfit.\n",
                "    if tolerance_counter >= 3:\n",
                "        break\n",
                "\n",
                "    tolerance_counter = tolerance_counter + 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict(text, text_length):\n",
                "    with torch.no_grad():\n",
                "        output = model(text, text_length).squeeze(1)\n",
                "        predicted = torch.max(output.data, 1)[1]\n",
                "        return predicted"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sentence_checking(test_list):\n",
                "    predict_list = list()\n",
                "    for sent in test_list:\n",
                "        text = torch.tensor(text_pipeline(sent)).to(device)\n",
                "        text_list = [x.item() for x in text]\n",
                "        text = text.reshape(1, -1)\n",
                "        text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)\n",
                "        predict_list.append(predict(text, text_length))\n",
                "    return predict_list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[tensor([2], device='cuda:0'), tensor([1], device='cuda:0'), tensor([2], device='cuda:0'), tensor([2], device='cuda:0')]\n"
                    ]
                }
            ],
            "source": [
                "#[\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
                "test_case = ['The movie should have been good', # Negative\n",
                "    'What is not to like about this product.', # Negative\n",
                "    \"The price is not so bad\", # Positive\n",
                "    'This software is not buggy'] # Positive\n",
                "\n",
                "print(sentence_checking(test_case))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ai50",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.15"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "da867d72de60a3e86a2b69a9a7baea090d67382d01a73f765a7401ae7e7cc0f6"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
