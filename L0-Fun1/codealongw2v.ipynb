{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['apple', 'banana', 'fruit'], ['banana', 'apple', 'fruit'], ['banana', 'fruit', 'apple'], ['dog', 'cat', 'animal'], ['cat', 'dog', 'animal'], ['cat', 'animal', 'dog']]\n",
      "*********************************************\n",
      "['fruit', 'cat', 'dog', 'apple', 'animal', 'banana']\n"
     ]
    }
   ],
   "source": [
    "# specify the sentences /corpus\n",
    "\n",
    "corpus = ['apple banana fruit', 'banana apple fruit', 'banana fruit apple','dog cat animal', 'cat dog animal','cat animal dog']\n",
    "\n",
    "# tokenize \n",
    "corpus_tokened = [sent.split(' ') for sent in corpus]\n",
    "print(corpus_tokened)\n",
    "\n",
    "# numericalize\n",
    "faltten_func = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "print('***'*15)\n",
    "vocabs = list(set(faltten_func(corpus_tokened)))\n",
    "print(vocabs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign id to all these vocab\n",
    "word2idx = {v:idx for idx,v in enumerate(vocabs)}\n",
    "vocabs.append('<UNK>')\n",
    "word2idx['<UNK>'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fruit': 0, 'cat': 1, 'dog': 2, 'apple': 3, 'animal': 4, 'banana': 5, '<UNK>': -1}\n",
      "{0: 'fruit', 1: 'cat', 2: 'dog', 3: 'apple', 4: 'animal', 5: 'banana', -1: '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx)\n",
    "print(idx2word)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prep train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ## move along the corpus \n",
    "\n",
    "# # skipgrams = []\n",
    "# # for sent in corpus_tokened:\n",
    "# #     for i in range(1,len(sent)-1):\n",
    "# #         center_word = sent[i]\n",
    "# #         outside_word = [sent[i-1],sent[i+1]]\n",
    "# #         for o in outside_word:\n",
    "# #             skipgrams.append([center_word,o]) \n",
    "# def random_batch(batch_size, word_sequence):\n",
    "    \n",
    "#     # Make skip gram of one size window\n",
    "#     skip_grams = []\n",
    "#     # loop each word sequencei\n",
    "#     # we starts from 1 because 0 has no context\n",
    "#     # we stop at second last for the same reason\n",
    "#     for sent in corpus:\n",
    "#         for i in range(1, len(sent) - 1):\n",
    "#             target = word2idx[sent[i]]\n",
    "#             context = [word2idx[sent[i - 1]], word2idx[sent[i + 1]]]\n",
    "#             for w in context:\n",
    "#                 skip_grams.append([target, w])\n",
    "    \n",
    "#     random_inputs = []\n",
    "#     random_labels = []\n",
    "#     random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "#     for i in random_index:\n",
    "#         random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "#         random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "#     return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(skipgrams)\n",
    "#let's make what we have made into a function (batch function)\n",
    "#return a batches of data, e.g., =2 --> ['banana', 'apple'], ['banana', 'fruit']\n",
    "#also i want these batches to be id, NOT token   --> [5, 4]\n",
    "\n",
    "def random_batch(batch_size, corpus_tokenized,word2index = word2idx):\n",
    "    \n",
    "    skipgrams = []\n",
    "\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "        for i in range(1, len(sent) - 1): #start from 1 to second last\n",
    "            center_word = word2index[sent[i]]\n",
    "            outside_words = [word2index[sent[i-1]], word2index[sent[i+1]]]  #window_size = 1\n",
    "            for o in outside_words:\n",
    "                skipgrams.append([center_word, o])\n",
    "                \n",
    "    #only get a batch, not the entire list\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "             \n",
    "    #appending some list of inputs and labels\n",
    "    random_inputs, random_labels = [], []   \n",
    "    for index in random_index:\n",
    "        random_inputs.append([skipgrams[index][0]])  #center words, this will be a shape of (1, ) --> (1, 1) for modeling\n",
    "        random_labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [5],\n",
       "        [5],\n",
       "        [4],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]]),\n",
       " array([[1],\n",
       "        [1],\n",
       "        [5],\n",
       "        [0],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2],\n",
       "        [4],\n",
       "        [3],\n",
       "        [2]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_batch(10, corpus_tokened,word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input=array([[2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [2],\n",
      "       [3]])\n",
      "label=array([[1],\n",
      "       [5],\n",
      "       [1],\n",
      "       [4],\n",
      "       [0]])\n"
     ]
    }
   ],
   "source": [
    "input, label = random_batch(5, corpus_tokened,word2idx)\n",
    "\n",
    "print(f\"{input=}\")\n",
    "print(f\"{label=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #the model will accept three vectors - u_o, v_c, u_w\n",
    "# #u_o - vector for outside words\n",
    "# #v_c - vector for center word\n",
    "# #u_w - vectors of all vocabs\n",
    "\n",
    "# class Skipgram(nn.Module):\n",
    "    \n",
    "#     def __init__(self, voc_size, emb_size):\n",
    "#         super(Skipgram, self).__init__()\n",
    "#         self.embedding_center_word = nn.Embedding(voc_size, emb_size)  #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "    \n",
    "#     def forward(self, center_word, outside_word):\n",
    "#         #center_word, outside_word: (batch_size, 1)\n",
    "#         #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "#         #convert them into embedding\n",
    "#         center_word_embed = self.embedding_center_word(center_word)   #(batch_size, 1, emb_size)\n",
    "        \n",
    "#         return center_word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vector for outside words\n",
    "#v_c - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)  #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size, 1)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.embedding_center_word(center_word)     #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)   #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embed   = self.embedding_outside_word(all_vocabs)     #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "         #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "         \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #sum exp(uw vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp / lower_term_sum))\n",
    "        #(batch_size, 1) / (batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing all_vocabs\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da867d72de60a3e86a2b69a9a7baea090d67382d01a73f765a7401ae7e7cc0f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
