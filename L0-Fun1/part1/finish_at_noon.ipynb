{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.22.0', '1.13.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the sentences / corpus\n",
    "#corpus is defined as a set of documents\n",
    "#document is basically a bunch of sentence(s)\n",
    "corpus = [\"apple banana fruit\", \"banana apple fruit\", \"banana fruit apple\", \n",
    "          \"dog cat animal\", \"cat dog animal\", \"cat animal dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'dog', 'animal'],\n",
       " ['cat', 'animal', 'dog']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. tokenize\n",
    "#usually you use spaCy / NLTK to tokenize (but we gonna do this later on, we gonna have spaCy)\n",
    "corpus_tokenized = [sent.split(\" \") for sent in corpus]\n",
    "corpus_tokenized  #we called each of this as \"tokens\", NOT words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numericalize\n",
    "\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten this (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs  = list(set(flatten(corpus_tokenized)))  #vocabs is a term defining all unique words your system know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add <UNK>, which is a very normal token exists in the world\n",
    "vocabs.append('<UNK>') #chaky, can it be ##UNK, or UNKKKKKK, or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have a way to know what is the id of <UNK>\n",
    "word2index['<UNK>'] = 6  #usually <UNK> is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'animal',\n",
       " 1: 'fruit',\n",
       " 2: 'apple',\n",
       " 3: 'dog',\n",
       " 4: 'banana',\n",
       " 5: 'cat',\n",
       " 6: '<UNK>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create index2word dictionary\n",
    "#2 min    \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animal', 'fruit', 'apple', 'dog', 'banana', 'cat', '<UNK>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data\n",
    "You move the window along, and create those tuples as we said in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['banana', 'apple'],\n",
       " ['banana', 'fruit'],\n",
       " ['apple', 'banana'],\n",
       " ['apple', 'fruit'],\n",
       " ['fruit', 'banana'],\n",
       " ['fruit', 'apple'],\n",
       " ['cat', 'dog'],\n",
       " ['cat', 'animal'],\n",
       " ['dog', 'cat'],\n",
       " ['dog', 'animal'],\n",
       " ['animal', 'cat'],\n",
       " ['animal', 'dog']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move along the corpus\n",
    "#to fit with our corpus, we gonna use window_size = 1\n",
    "\n",
    "skipgrams = []\n",
    "\n",
    "#for each corpus\n",
    "for sent in corpus_tokenized:\n",
    "    #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "    for i in range(1, len(sent) - 1): #start from 1 to second last\n",
    "        center_word = sent[i]\n",
    "        outside_words = [sent[i-1], sent[i+1]]  #window_size = 1\n",
    "        for o in outside_words:\n",
    "            skipgrams.append([center_word, o])\n",
    "\n",
    "skipgrams\n",
    "        \n",
    "#here we want to create (banana, apple), (banana, fruit) append to some list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make what we have made into a function (batch function)\n",
    "#return a batches of data, e.g., =2 --> ['banana', 'apple'], ['banana', 'fruit']\n",
    "#also i want these batches to be id, NOT token   --> [5, 4]\n",
    "\n",
    "def random_batch(batch_size, corpus):\n",
    "    \n",
    "    skipgrams = []\n",
    "\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "        for i in range(1, len(sent) - 1): #start from 1 to second last\n",
    "            center_word = word2index[sent[i]]\n",
    "            outside_words = [word2index[sent[i-1]], word2index[sent[i+1]]]  #window_size = 1\n",
    "            for o in outside_words:\n",
    "                skipgrams.append([center_word, o])\n",
    "                \n",
    "    #only get a batch, not the entire list\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "             \n",
    "    #appending some list of inputs and labels\n",
    "    random_inputs, random_labels = [], []   \n",
    "    for index in random_index:\n",
    "        random_inputs.append([skipgrams[index][0]])  #center words, this will be a shape of (1, ) --> (1, 1) for modeling\n",
    "        random_labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "label=array([[3],\n",
      "       [4],\n",
      "       [1],\n",
      "       [0],\n",
      "       [2],\n",
      "       [2],\n",
      "       [5],\n",
      "       [1],\n",
      "       [3],\n",
      "       [4]])\n"
     ]
    }
   ],
   "source": [
    "input, label = random_batch(10, corpus_tokenized)\n",
    "\n",
    "print(f\"{input.shape}\")\n",
    "print(f\"{label=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size = len(vocabs)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animal', 'fruit', 'apple', 'dog', 'banana', 'cat', '<UNK>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vector for outside words\n",
    "#v_c - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)  #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size, 1)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.embedding_center_word(center_word)     #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)   #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embed   = self.embedding_outside_word(all_vocabs)     #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "         #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "         \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #sum exp(uw vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp / lower_term_sum))\n",
    "        #(batch_size, 1) / (batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing all_vocabs\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = random_batch(batch_size, corpus_tokenized)\n",
    "input #center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [4]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label #context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2 #usually, this can be 50, 100, or 300\n",
    "model = Skipgram(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(input)  \n",
    "label_tensor = torch.LongTensor(label)  #LongTensor basically means integer...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is different\n",
    "torch.LongTensor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([2])  #put shape (, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6],\n",
       "        [0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should give one number\n",
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2927, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #why?  no reason; \n",
    "emb_size   = 2 #why?  no reason; usually 50, 100, 300, but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model      = Skipgram(voc_size, emb_size)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss()  #-log\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 1.970407 | Time: ??\n",
      "Epoch 2000 | Loss: 0.347920 | Time: ??\n",
      "Epoch 3000 | Loss: 1.103766 | Time: ??\n",
      "Epoch 4000 | Loss: 0.812391 | Time: ??\n",
      "Epoch 5000 | Loss: 1.246513 | Time: ??\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model(input_batch, label_batch, all_vocabs)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: ??\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the embeddings\n",
    "\n",
    "Is really the related stuff are close to each other, and vice versa?\n",
    "\n",
    "The most fun part:  Will \"banana\" closer to \"fruit\" than \"cat\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animal', 'fruit', 'apple', 'dog', 'banana', 'cat', '<UNK>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.6747, -0.2560]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_center_embed = model.embedding_center_word(banana)\n",
    "banana_outisde_embed = model.embedding_outside_word(banana)\n",
    "\n",
    "banana_embed = (banana_center_embed + banana_outisde_embed) / 2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_center_word(word)\n",
    "    outside_embed = model.embedding_outside_word(word)\n",
    "    \n",
    "    embed = (center_embed + outside_embed) / 2\n",
    "    \n",
    "    return  embed[0][0].item(), embed[0][1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2.612426280975342, 0.19476547837257385)\n",
      "(2.9470579624176025, 1.7921385765075684)\n",
      "(0.31564128398895264, 0.034840065985918045)\n"
     ]
    }
   ],
   "source": [
    "#find embedding of fruit, cat\n",
    "print(get_embed('fruit'))\n",
    "print(get_embed('cat'))\n",
    "\n",
    "print(get_embed('chaky'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#help me plot fruit cat banana on matplotlib\n",
    "plt.figure(figsize=(6,3))\n",
    "for i, word in enumerate(vocabs[:20]): #loop each unique vocab\n",
    "    x, y = get_embed(word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine similarity\n",
    "\n",
    "How do (from scratch) calculate cosine similarity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da867d72de60a3e86a2b69a9a7baea090d67382d01a73f765a7401ae7e7cc0f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
