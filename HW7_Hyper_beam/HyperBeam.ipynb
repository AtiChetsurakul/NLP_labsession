{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let to **rayquaza** use HyperBeam\n",
    "\n",
    "You guys probably very excited about ChatGPT.  In today class, we will be implementing a very simple language model, which is basically what ChatGPT is, but with a simple LSTM.  You will be surprised that it is not so difficult at all.\n",
    "\n",
    "Paper that we base on is *Regularizing and Optimizing LSTM Language Models*, https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "import re\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    " # this is the best seed for training to get the best result\n",
    "torch.manual_seed(3407)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Wiki Text\n",
    "\n",
    "We will be using wikitext which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1\n",
      "Found cached dataset parquet (/home/atichets/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Using custom data configuration codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1\n",
      "Found cached dataset parquet (/home/atichets/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text/tree/main dataset\n",
    "#there are raw and preprocessed version; we used the raw one and preprocessed ourselves for fun\n",
    "train_all_set= datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\", split=\"train\")\n",
    "test_all_set = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\", split=\"test\")\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# print(train_jupyter, test_jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set = [split for text in train_all_set['content'] for split in text.split('\\n') if split != \"\"]\n",
    "test_set = [split for text in test_all_set['content'] for split in text.split('\\n') if split != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11367363, 2875424)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set),len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = 300000\n",
    "test_total = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "tokenized_dataset_train = yield_tokens(train_set[train_total:2*train_total])\n",
    "tokenized_dataset_test = yield_tokens(test_set[:test_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    \n",
    "    # Clear the html tag by using regular expression.\n",
    "    sentence = re.sub(\"<[^>]*>\", \"\", sentence)\n",
    "    sentence = re.sub(\"[^\\x00-\\x7F]+\", \"\", sentence) #extract non-english out\n",
    "    #It matches any character which is not contained in the ASCII character set (0-127, i.e. 0x0 to 0x7F)\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
    "            token.pos_ != 'SYM' and token.pos_!= 'X':\n",
    "                cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "                \n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENEW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(preprocessing(text))\n",
    "if RENEW:\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(train_set[:test_total]), min_freq=5) \n",
    "    vocab.insert_token('<unk>', 0)           \n",
    "    vocab.insert_token('<eos>', 3)            \n",
    "    vocab.insert_token('<sos>', 2)            \n",
    "    vocab.insert_token('<pad>', 1)            \n",
    "    vocab.set_default_index(vocab['<unk>'])   \n",
    "    print('Vocab Size',len(vocab))                         \n",
    "    print(vocab.get_itos()[:10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size check 393\n"
     ]
    }
   ],
   "source": [
    "if RENEW:\n",
    "    with open('vocabjjngu.txt', 'w') as file:\n",
    "        for item in vocab.get_itos():\n",
    "            # write each item on a new line\n",
    "            file.write(\"%s\\n\" % item)\n",
    "        print('Done')\n",
    "else: \n",
    "    v = [line.rstrip() for line in open('vocabjjngu.txt', mode = 'r')]\n",
    "    print('Vocab Size check', len(v)) #not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RENEW:\n",
    "    with open('vocabjjngu.atikeep', 'wb') as handle:\n",
    "        pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('vocabjjngu.atikeep', 'rb') as handle:\n",
    "        vocab = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:       \n",
    "        #appends eos so we know it ends....so model learn how to end...                             \n",
    "        tokens = example.append('<eos>') #end of sentence\n",
    "        #numericalize          \n",
    "        tokens = [vocab[token] for token in example] \n",
    "        data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "train_data = get_data(tokenized_dataset_train, vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset_test, vocab, batch_size)\n",
    "# test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 232160])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape #[batch_size, all the next length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper function to yield list of tokens\n",
    "# # here data can be `train` or `val` or `test`\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# token_transform = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "# def yield_tokens(data):\n",
    "#     for data_sample in data:\n",
    "#         yield token_transform(data_sample) #either first or second index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "#     # Create torchtext's Vocab object \n",
    "# vocab_transform = build_vocab_from_iterator(yield_tokens(train_data), \n",
    "#                                                     min_freq=2,   #if not, everything will be treated as UNK\n",
    "#                                                     specials=special_symbols,\n",
    "#                                                     special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# # Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "\n",
    "# vocab_transform.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# BATCH_SIZE = 12\n",
    "# UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# # helper function to club together sequential operations\n",
    "# def sequential_transforms(*transforms):\n",
    "#     def func(txt_input):\n",
    "#         for transform in transforms:\n",
    "#             txt_input = transform(txt_input)\n",
    "#         return txt_input\n",
    "#     return func\n",
    "\n",
    "# # function to add BOS/EOS and create tensor for input sequence indices\n",
    "# def tensor_transform(token_ids):\n",
    "#     return torch.cat((torch.tensor([SOS_IDX]), \n",
    "#                       torch.tensor(token_ids), \n",
    "#                       torch.tensor([EOS_IDX])))\n",
    "\n",
    "# # src and trg language text transforms to convert raw strings into tensors indices\n",
    "# # text_transform = {}\n",
    "# # for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "# text_transform = sequential_transforms(tokenizer, #Tokenization\n",
    "#                                         vocab, #Numericalization\n",
    "#                                         tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# # function to collate data samples into batch tesors\n",
    "# def collate_batch(batch):\n",
    "#     src_batch, src_len_batch = [], []\n",
    "#     for src_sample in batch:\n",
    "#         processed_text = text_transform(src_sample.rstrip(\"\\n\"))\n",
    "#         src_batch.append(processed_text)\n",
    "#         # trg_batch.append(text_transform(trg_sample.rstrip(\"\\n\")))\n",
    "#         src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "#     src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "#     # trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "#     return src_batch, torch.tensor(src_len_batch, dtype=torch.int64)#,# trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 26\n",
    "\n",
    "train_loader = DataLoader(train_set[train_total:2*train_total], batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "# valid_loader = DataLoader(val, batch_size=batch_size,\n",
    "#                                shuffle=True, collate_fn=collate_batch)\n",
    "# test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "#                              shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for en,baz in train_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([75, 26])\n",
      "German shape:  torch.Size([26])\n"
     ]
    }
   ],
   "source": [
    "# print(\"first shape: \", en.shape)  # (seq len, batch_size)\n",
    "# print(\"sec shape: \", baz.shape)   # (seq len, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hid_dim):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "#         self.W = nn.Linear(hid_dim,     hid_dim) #for decoder\n",
    "#         self.U = nn.Linear(hid_dim * 2, hid_dim) #for encoder outputs\n",
    "                \n",
    "#     def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "#         #hidden = [batch size, hid dim]\n",
    "#         #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "#         batch_size = encoder_outputs.shape[1]\n",
    "#         src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "#         #repeat decoder hidden state src_len times\n",
    "#         hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "#         #hidden = [batch size, src len, hid dim]\n",
    "\n",
    "#         encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "#         #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        \n",
    "#         energy = torch.tanh(self.W(hidden) + self.U(encoder_outputs))\n",
    "#         #energy = [batch size, src len, hid dim]\n",
    "        \n",
    "#         attention = self.v(energy).squeeze(2)\n",
    "#         #attention = [batch size, src len]\n",
    "        \n",
    "#         #use masked_fill_ if you want in-place\n",
    "#         attention = attention.masked_fill(mask, -1e10)\n",
    "        \n",
    "#         return F.softmax(attention, dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "        self.W = nn.Linear(hid_dim, hid_dim)\n",
    "                \n",
    "    def forward(self, hidden, mask):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        # batch_size = hidden.shape[0]\n",
    "        # src_len = hidden.shape[1]\n",
    "        return F.softmax(self.v(torch.tanh(self.W(hidden))),dim=0)         # attention = attention.masked_fill(mask, -1e10)  #<---- This still Bug\n",
    "        # hidden = hidden.permute(1, 0)\n",
    "        #hidden = [src len, batch size, hid dim]\n",
    "        # print('here')\n",
    "        # print(hidden.size())\n",
    "        energy = torch.tanh(self.W(hidden))\n",
    "        #energy = [src len, batch size, hid dim]\n",
    "        # print('here 2')\n",
    "        # print(energy.size())\n",
    "        attention = self.v(energy)#.squeeze(2)\n",
    "        #attention = [src len, batch size]\n",
    "        # print('herte3')\n",
    "        #use masked_fill_ if you want in-place\n",
    "        # print(attention.size())\n",
    "        # attention = attention.masked_fill(mask, -1e10)\n",
    "        # print(attention.size())\n",
    "        # print('here4')\n",
    "        return F.softmax(attention, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        # a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch size, src len]\n",
    "        # print('hgfd')\n",
    "        # print(hidden.size(),mask.size())\n",
    "        a = self.attention(hidden, mask)\n",
    "        # print('aetter')    \n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        # print(a.size())\n",
    "        # encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        encoder_outputs = torch.ones((a.size(0),a.size(1),self.hid_dim*2)).to(self.device)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted = [batch size, 1, hid dim * 2]\n",
    "        # print(weighted.size())\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted = [1, batch size, hid dim * 2]\n",
    "        # print(weighted.size())\n",
    "        # print(embedded.size())\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input = [1, batch size, (hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        self.h        = hiddenstate  #define the hidden state\n",
    "        self.prevNode = previousNode  #where does it come from\n",
    "        self.wordid   = wordId  #the numericalized integer of the word\n",
    "        self.logp     = logProb  #the log probability\n",
    "        self.len      = length  #the current length; first word starts at 1\n",
    "\n",
    "    def eval(self, alpha=0.7):\n",
    "        # the score will be simply the log probability penaltized by the length \n",
    "        # we add some small number to avoid division error\n",
    "        # read https://arxiv.org/abs/1808.10006 to understand how alpha is selected\n",
    "        return self.logp / float(self.len + 1e-6) ** (alpha)\n",
    "    \n",
    "    #this is the function for comparing between two beamsearchnodes, whether which one is better\n",
    "    #it is called when you called \"put\"\n",
    "    def __lt__(self, other):\n",
    "        return self.len < other.len\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.len > other.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "import operator\n",
    "\n",
    "class Seq2SeqBeam(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)  #permute so it's the same shape as attention\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is the probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.5 we use teacher forcing 50% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #tensor to store attentiont outputs from decoder\n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        # encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        encoder_outputs,hidden  = [],self.init_hidden(src)\n",
    "        # print(hidden.shape)\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input_ = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            # print('before')\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            # print('after')\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #place attentions in a tensor holding attention for each token\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input_ = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions\n",
    "    \n",
    "    \n",
    "    #use during inference\n",
    "    #encapsulates beam_decode or greedy_decode\n",
    "    def decode(self, src, src_len, trg, method='beam-search'):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #src len = [batch size]\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len) \n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]  (*2 because of bidirectional)(every hidden states)\n",
    "        #hidden = [batch size, hid dim]  #final hidden state\n",
    "       \n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        if method == 'beam-search':\n",
    "            return self.beam_decode(src, trg, hidden, encoder_outputs)\n",
    "        else:\n",
    "            return self.greedy_decode(trg, hidden, encoder_outputs)\n",
    "\n",
    "    def greedy_decode(self, trg, decoder_hidden, encoder_outputs):\n",
    "            # trg = [trg_len, batch_size]\n",
    "        # decoder_hidden = [1, batch_size, hid_dim]\n",
    "        # encoder_outputs = [src_len, batch_size, hid_dim * 2]\n",
    "        \n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        decoded_outputs = torch.zeros((trg_len, batch_size)).to(self.device)\n",
    "        \n",
    "        # First input to the decoder is the <sos> tokens\n",
    "        input_ = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            output, decoder_hidden, _ = self.decoder(input_.unsqueeze(0), decoder_hidden, encoder_outputs)\n",
    "            decoded = output.argmax(1)\n",
    "            decoded_outputs[t] = decoded\n",
    "            input_ = decoded\n",
    "        \n",
    "        # Transpose the output from [trg_len, batch_size] to [batch_size, trg_len]\n",
    "        return decoded_outputs.permute(1, 0)\n",
    "        pass\n",
    "\n",
    "    def beam_decode(self, src_tensor, target_tensor, decoder_hiddens, encoder_outputs=None):\n",
    "        #src_tensor      = [src len, batch size]\n",
    "        #target_tensor   = [trg len, batch size]\n",
    "        #decoder_hiddens = [1, batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "        target_tensor = target_tensor.permute(1, 0)\n",
    "        #target_tensor = [batch size, trg len]\n",
    "        \n",
    "        #how many parallel searches\n",
    "        beam_width = 3\n",
    "        \n",
    "        #how many sentence do you want to generate\n",
    "        topk = 1  \n",
    "        \n",
    "        #final generated sentence\n",
    "        decoded_batch = []\n",
    "                \n",
    "        #Another difference is that beam_search_decoding has \n",
    "        #to be done sentence by sentence, thus the batch size is indexed and reduced to only 1.  \n",
    "        #To keep the dimension same, we unsqueeze 1 dimension for the batch size.\n",
    "        for idx in range(target_tensor.size(0)):  # batch_size\n",
    "            \n",
    "            #decoder_hiddens = [1, batch size, dec hid dim]\n",
    "            decoder_hidden = decoder_hiddens[:, idx, :]\n",
    "            #decoder_hidden = [1, dec hid dim]\n",
    "            \n",
    "            #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "            encoder_output = encoder_outputs[:, idx, :].unsqueeze(1)\n",
    "            #encoder_output = [src len, 1, enc hid dim * 2]\n",
    "            \n",
    "            mask = self.create_mask(src_tensor[:, idx].unsqueeze(1))\n",
    "            # print(\"mask shape: \", mask.shape)\n",
    "            \n",
    "            #mask = [1, src len]\n",
    "\n",
    "            # Start with the start of the sentence token\n",
    "            decoder_input = torch.LongTensor([SOS_IDX]).to(device)\n",
    "\n",
    "            # Number of sentence to generate\n",
    "            endnodes = []  #hold the nodes of EOS, so we can backtrack\n",
    "            number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "            # starting node -  hidden vector, previous node, word id, logp, length\n",
    "            node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
    "            nodes = PriorityQueue()  #this is a min-heap\n",
    "\n",
    "            # start the queue\n",
    "            nodes.put((-node.eval(), node))  #we need to put - because PriorityQueue is a min-heap\n",
    "            qsize = 1\n",
    "\n",
    "            # start beam search\n",
    "            while True:\n",
    "                # give up when decoding takes too long\n",
    "                if qsize > 2000: break\n",
    "\n",
    "                # fetch the best node\n",
    "                # score is log p divides by the length scaled by some constants\n",
    "                score, n = nodes.get()\n",
    "                            \n",
    "                # wordid is simply the numercalized integer of the word\n",
    "                decoder_input  = n.wordid\n",
    "                decoder_hidden = n.h\n",
    "\n",
    "                if n.wordid.item() == EOS_IDX and n.prevNode != None:\n",
    "                    endnodes.append((score, n))\n",
    "                    # if we reached maximum # of sentences required\n",
    "                    if len(endnodes) >= number_required:\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                # decode for one step using decoder\n",
    "                # decoder_input = SOS_IDX\n",
    "                # decoder_hidden = [1, hid dim]\n",
    "                # encoder_output = [src len, 1, hid dim * 2]\n",
    "                # mask = [1, src len]\n",
    "                \n",
    "                prediction, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output, mask)\n",
    "                #prediction     = [1, output dim]  #1 because the batch size is 1\n",
    "                #decoder hidden = [1, hid dim]\n",
    "\n",
    "                #so basically prediction is probabilities across all possible vocab\n",
    "                #we gonna retrieve k top probabilities (which is defined by beam_width) and their indexes\n",
    "                #recall that beam_width defines how many parallel searches we want\n",
    "                log_prob, indexes = torch.topk(prediction, beam_width)\n",
    "                # log_prob      = (1, beam width)\n",
    "                # indexes       = (1, beam width)\n",
    "                \n",
    "                nextnodes = []  #the next possible node you can move to\n",
    "\n",
    "                # we only select beam_width amount of nextnodes\n",
    "                for top in range(beam_width):\n",
    "                    pred_t = indexes[0, top].reshape(-1)  #reshape because wordid is assume to be []; see when we define SOS\n",
    "                    log_p  = log_prob[0, top].item()\n",
    "                                    \n",
    "                    #decoder hidden, previous node, current node, prob, length\n",
    "                    node = BeamSearchNode(decoder_hidden, n, pred_t, n.logp + log_p, n.len + 1)\n",
    "                    score = -node.eval()\n",
    "                    nextnodes.append((score, node))\n",
    "\n",
    "                # put them into queue\n",
    "                for i in range(len(nextnodes)):\n",
    "                    score, nn = nextnodes[i]\n",
    "                    nodes.put((score, nn))\n",
    "                    # increase qsize\n",
    "                qsize += len(nextnodes) - 1\n",
    "\n",
    "            # Once everything is finished, choose nbest paths, back trace them\n",
    "            \n",
    "            ## in case it does not finish, we simply get couple of nodes with highest probability\n",
    "            if len(endnodes) == 0:\n",
    "                endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "            #look from the end and go back....\n",
    "            utterances = []\n",
    "            for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "                utterance = []\n",
    "                utterance.append(n.wordid)\n",
    "                # back trace by looking at the previous nodes.....\n",
    "                while n.prevNode != None:\n",
    "                    n = n.prevNode\n",
    "                    utterance.append(n.wordid)\n",
    "\n",
    "                utterance = utterance[::-1]  #reverse it....\n",
    "                utterances.append(utterance) #append to the list of sentences....\n",
    "\n",
    "            decoded_batch.append(utterances)\n",
    "\n",
    "        return decoded_batch  #(batch size, length)\n",
    "    \n",
    "    def init_hidden(self,src):\n",
    "        #this function gonna be run in the beginning of the epoch\n",
    "        hidden = torch.zeros( src.size(0),self.decoder.hid_dim).to(self.device)\n",
    "        return hidden #return as tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqBeam(\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(393, 256)\n",
       "    (gru): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=393, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab)\n",
    "output_dim  = len(vocab)\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = Attention(hid_dim)\n",
    "# enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn,device)\n",
    "\n",
    "model = Seq2SeqBeam('', dec, SRC_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class LSTMLanguageModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "                \n",
    "#         super().__init__()\n",
    "#         self.hid_dim = hid_dim\n",
    "#         self.num_layers = num_layers\n",
    "#         self.embedding = nn.Embedding(vocab_size,emb_dim)\n",
    "#         self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
    "#                                         dropout = dropout_rate, batch_first = True)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         #when you do LM, you look forward, so it does not make sense to do bidirectional\n",
    "#         self.fc = nn.Linear(hid_dim,vocab_size)\n",
    "\n",
    "#     def init_hidden(self, batch_size, device):\n",
    "#         #this function gonna be run in the beginning of the epoch\n",
    "#         hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "#         cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "\n",
    "#         return hidden, cell #return as tuple\n",
    "\n",
    "#     def detach_hidden(self, hidden):\n",
    "#         hidden, cell = hidden\n",
    "#         hidden = hidden.detach() #removing this hidden from gradients graph\n",
    "#         cell =  cell.detach() #removing this hidden from gradients graph\n",
    "#         return hidden, cell\n",
    "\n",
    "#     def forward(self, src, hidden):\n",
    "#         #src: [batch_size, seq_len]\n",
    "\n",
    "#         #embed \n",
    "#         embedded = self.embedding(src)\n",
    "#         #embed : [batch_size, seq_len, emb_dim]\n",
    "\n",
    "#         #send this to the lstm\n",
    "#         #we want to put hidden here... because we want to reset hidden .....\n",
    "#         output, hidden = self.lstm(embedded, hidden)\n",
    "#         #output : [batch_size, seq_len, hid_dim] ==> all hidden states\n",
    "#         #hidden : [batch_size, seq_len, hid_dim] ==> last hidden states from each layer\n",
    "\n",
    "#         output = self.dropout(output)\n",
    "#         prediction = self.fc(output)\n",
    "#         #prediction: [batch size, seq_len, vocab_size]\n",
    "#         return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(vocab)\n",
    "# emb_dim = 400                # 400 in the paper\n",
    "# hid_dim = 1150               # 1150 in the paper\n",
    "# num_layers = 3                # 3 in the paper\n",
    "# dropout_rate = 0.5           \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,824,009 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #this data is from get_data()\n",
    "    #train_data.shape # [batch_size, number of batches....]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        # hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        hidden = model.init_hidden(src)\n",
    "        prediction, hidden = model(src, 0,target)               \n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) #prevent gradient explosion - clip is basically \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    # hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            # hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src,0, target)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 1\n",
      "\tTrain Perplexity: 7.729\n",
      "\tValid Perplexity: 9.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 2\n",
      "\tTrain Perplexity: 7.736\n",
      "\tValid Perplexity: 9.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 3\n",
      "\tTrain Perplexity: 7.705\n",
      "\tValid Perplexity: 9.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m best_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 10\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_data, optimizer, criterion, \n\u001b[1;32m     11\u001b[0m                 batch_size, seq_len, clip, device)\n\u001b[1;32m     12\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_data, criterion, batch_size, \n\u001b[1;32m     13\u001b[0m                 seq_len, device)\n\u001b[1;32m     15\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep(valid_loss)\n",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m batch_size \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden(src)\n\u001b[0;32m---> 20\u001b[0m prediction, hidden \u001b[39m=\u001b[39m model(src, \u001b[39m0\u001b[39;49m,target)               \n\u001b[1;32m     22\u001b[0m prediction \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39mreshape(batch_size \u001b[39m*\u001b[39m seq_len, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m#prediction: [batch size * seq len, vocab size]  \u001b[39;00m\n\u001b[1;32m     23\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/JAN23/ai50/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[23], line 52\u001b[0m, in \u001b[0;36mSeq2SeqBeam.forward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m#mask = [batch size, src len]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, trg_len):\n\u001b[1;32m     47\u001b[0m     \n\u001b[1;32m     48\u001b[0m     \u001b[39m#insert input token embedding, previous hidden state, all encoder hidden states \u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39m#  and mask\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39m#receive output tensor (predictions) and new hidden state\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[39m# print('before')\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     output, hidden, attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(input_, hidden, encoder_outputs, mask)\n\u001b[1;32m     53\u001b[0m     \u001b[39m# print('after')\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39m#place predictions in a tensor holding predictions for each token\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     outputs[t] \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m~/Desktop/JAN23/ai50/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[21], line 49\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m rnn_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((embedded, weighted), dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39m#rnn_input = [1, batch size, (hid dim * 2) + emb dim]\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(rnn_input, hidden\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     50\u001b[0m \u001b[39m#output = [seq len, batch size, dec hid dim * n directions]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m#hidden = [n layers * n directions, batch size, hid dim]\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m#hidden = [1, batch size, hid dim]\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m#this also means that output == hidden\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39massert\u001b[39;00m (output \u001b[39m==\u001b[39m hidden)\u001b[39m.\u001b[39mall()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "seq_len  = 12\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'predictor_weight.pt')\n",
    "    print(f'\\tepoch: {epoch+1}')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('predictor_weight.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=3407):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    # hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "\n",
    "            prediction, hidden = model(src, 0,src*3)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "<unk> line run_setting str learning latency 50hertz value epoch in renewable how='outer signature dense m1 sgd measure determine index sheet add need client setting image notebook want coordinate function analysis product\n",
      "\n",
      "0.7\n",
      "<unk> line run_setting str learning latency 50hertz value epoch in renewable how='outer signature dense m1 sgd measure determine index sheet add need client setting image notebook want coordinate function analysis product\n",
      "\n",
      "0.75\n",
      "<unk> line run_setting str learning latency 50hertz value epoch in renewable how='outer signature dense m1 sgd measure determine index sheet add need client setting image notebook want coordinate function analysis product\n",
      "\n",
      "0.8\n",
      "<unk> line run_setting str learning latency 50hertz value epoch in renewable how='outer signature dense m1 sgd measure determine index sheet add need client setting image notebook want coordinate function analysis product\n",
      "\n",
      "1.0\n",
      "<unk> line run_setting str learning latency 50hertz value epoch in renewable how='outer signature dense m1 sgd measure determine index sheet add need client setting image notebook want coordinate function analysis product\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'generate'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "        #superdiverse   more diverse\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0] \n",
    "#sample from this distribution higher probability will get more change\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a27209d6372dbf41e999645d9358c6819e45693e132235eb8d4f4130c00c0509"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
