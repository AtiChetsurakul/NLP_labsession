{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 0 making corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with 103 intilegence%, I would like to use a bs4 to sracp text from how to web-scraping web blog \n",
    "#### **ref** : 100D python by angelar and towardsdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    URL = 'https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c'\n",
    "    response = requests.get(URL)\n",
    "    website_html = response.text\n",
    "    soup = BeautifulSoup(website_html, \"html.parser\")\n",
    "    all_paragraph = soup.find_all(name=\"p\", class_=\"pw-post-body-paragraph\")\n",
    "    get_only_text = [para.getText() for para in all_paragraph]\n",
    "    # print(len(get_only_text)) # 40 quite enough\n",
    "    my_corpus = [text[:80] for text in get_only_text if len(text) > 40 ] \n",
    "    with open('hijackdata.atikeep','wb') as tostore:\n",
    "        pickle.dump(my_corpus,tostore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all document =  38 doc, and each document have around 80 characters\n"
     ]
    }
   ],
   "source": [
    "with open('hijackdata.atikeep','rb') as readed:\n",
    "    my_corpus = pickle.load(readed)\n",
    "print('all document = ',len(my_corpus),'doc, and each document have around',len(my_corpus[-1]),'characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We always say “Garbage in Garbage out” in data science. If you do not have good  with char =  80\n",
      "Web Scraping is an automatic way to retrieve unstructured data from a website an with char =  80\n",
      "Scraping makes the website traffic spike and may cause the breakdown of the webs with char =  80\n",
      "You can see that Google does not allow web scraping for many of its sub-websites with char =  80\n",
      "Another note is that you can see from the first row on User-agent. Here Google s with char =  80\n",
      "Web scraping just works like a bot person browsing different pages website and c with char =  80\n",
      "Alright, finally we are here. There are 2 different approaches for web scraping  with char =  80\n",
      "Pros and Cons for this approach: It is simple and direct. However, if the websit with char =  80\n",
      "Pros and Cons for this approach: It is definitely a preferred approach if you ca with char =  80\n",
      "There are many different scraping tools available that do not require any coding with char =  80\n",
      "The most commonly used library for web scraping in Python is Beautiful Soup, Req with char =  80\n",
      "Beautiful Soup: It helps you parse the HTML or XML documents into a readable for with char =  80\n",
      "Requests: It is a Python module in which you can send HTTP requests to retrieve  with char =  80\n",
      "Selenium: It is widely used for website testing and it allows you to automate di with char =  80\n",
      "You can either use Requests + Beautiful Soup or Selenium to do web scraping. Sel with char =  80\n",
      "Step 1: Inspect the website(if using Chrome you can right-click and select inspe with char =  80\n",
      "I can see that data I need are all wrap in the HTML element with the unique clas with char =  80\n",
      "Step 2: Access URL of the website using code and download all the HTML contents  with char =  80\n",
      "I used the requests library to get data from a website. You can see that so far  with char =  80\n",
      "Step 3: Format the downloaded content into a readable format with char =  60\n",
      "This step is very straightforward and what we do is just parse unstructured text with char =  80\n",
      "The output is a much more readable format and you can search different HTML elem with char =  80\n",
      "Step 4: Extract out useful information and save it into a structured format with char =  75\n",
      "This step requires some time to understand website structure and find out where  with char =  80\n",
      "I created 5 different lists to store the different fields of data that I need. I with char =  80\n",
      "Step 5: For information displayed on multiple pages of the website, you may need with char =  80\n",
      "If you want to scrape all the data. Firstly you should find out about the total  with char =  80\n",
      "Step 1: Inspect the XHR network section of the URL that you want to crawl and fi with char =  80\n",
      "I can see from the Network that all product information is listed in this API ca with char =  80\n",
      "Step 2: Depending on the type of request(post or get) and also the request heade with char =  80\n",
      "Here I create the HTTP POST request using the requests library. For post request with char =  80\n",
      "Another thing to note here is that inside the payload, I specified limit as 100  with char =  80\n",
      "Step 3: Extract out useful information that you need with char =  52\n",
      "Data from API is usually quite neat and structured and thus what I did was just  with char =  80\n",
      "Step 4: For API with a limit on query size, you will need to use ‘for loop’ to r with char =  80\n",
      "Here is the complete code to scrape all rows of face mask data in Ezbuy. I found with char =  80\n",
      "If you want to scrape a website, I would suggest checking the existence of API f with char =  80\n",
      "If you are interested to know more about web scraping using Scrapy in Python can with char =  80\n"
     ]
    }
   ],
   "source": [
    "for i in my_corpus:\n",
    "    print(i, 'with char = ',len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da867d72de60a3e86a2b69a9a7baea090d67382d01a73f765a7401ae7e7cc0f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
